Overview
This document describes the setup and usage of a Python-based test tool that simulates multiple client connections to an Amazon Aurora PostgreSQL database via RDS Proxy. It performs concurrent SELECT, INSERT, and UPDATE operations while monitoring and logging behavior during failover scenarios.

The script helps validate connection resilience, failover duration, and DML operation reliability under stress and simulated outages.

Features
Simulates multiple clients (like pgbench -c N) using Python threads.

Performs:

SELECT now()

INSERT INTO test_failover

UPDATE test_failover

Handles failover gracefully using automatic retry logic.

Tracks:

Connection errors

Reconnect events

DML failures and successes

Logs detailed events in a CSV file.

Prints a live summary of DML operation counts per client.

Requirements
Python 3.7+

Install dependencies:

bash
Copy
Edit
pip install psycopg2-binary
Setup Instructions
1. Aurora PostgreSQL & RDS Proxy
Ensure:

You have an Aurora PostgreSQL cluster (compatible with RDS Proxy).

RDS Proxy is created and points to the Aurora cluster.

The user used in this script has permissions:

CREATE TABLE, INSERT, UPDATE, SELECT

2. Create Test Table (auto-created by script if permissions allow)
sql
Copy
Edit
CREATE TABLE IF NOT EXISTS test_failover (
    id SERIAL PRIMARY KEY,
    client_id INT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
3. Update Script Configuration
Edit the following values in the script:

python
Copy
Edit
DB_CONFIG = {
    "host": "your-rds-proxy-endpoint.rds.amazonaws.com",
    "port": 5432,
    "user": "your_user",
    "password": "your_password",
    "dbname": "your_dbname",
}
You can also adjust:

CLIENT_COUNT: Number of concurrent simulated clients

QUERY_INTERVAL: Time between operations

RETRY_INTERVAL: Wait time before retry on failure

How to Run the Script
bash
Copy
Edit
python aurora_failover_dml_test.py
This will:

Create the test_failover table if it doesn't exist

Start CLIENT_COUNT client threads

Log all activity to failover_dml_log.csv

Periodically print a DML summary like this:

pgsql
Copy
Edit
===== DML Summary =====
Client-1: INSERT ✓ 12 ✗ 1 | UPDATE ✓ 12 ✗ 1
Client-2: INSERT ✓ 13 ✗ 0 | UPDATE ✓ 13 ✗ 0
Client-3: INSERT ✓ 11 ✗ 2 | UPDATE ✓ 11 ✗ 2
=======================
Performing a Failover Test
Let the script run for 30–60 seconds.

Trigger a failover using one of the following:

Option A: AWS Console
Navigate to RDS → Aurora Cluster → Failover

Option B: AWS CLI
bash
Copy
Edit
aws rds failover-db-cluster --db-cluster-identifier <your-cluster-id>
Monitor the terminal and failover_dml_log.csv for:

query_failed

dml_failed

reconnect

Time taken between failure and recovery

Sample Output (CSV)
pgsql
Copy
Edit
timestamp,client,event_type,message
2025-04-02T17:00:20,Client-3,insert_success,Inserted one row
2025-04-02T17:00:21,Client-3,query_failed,server closed the connection unexpectedly
2025-04-02T17:00:22,Client-3,reconnect,Attempting to reconnect
2025-04-02T17:00:25,Client-3,connected,Connection established
Advanced Options (Optional Enhancements)
Customize SQL mix: Add DELETEs or complex JOINs

Store metrics in a database or send to Prometheus/Grafana

Use asyncio for higher concurrency performance

Wrap script as a CLI tool for configurable test plans


Script is here:


import psycopg2
import threading
import time
import csv
from datetime import datetime
from collections import defaultdict

# Configuration
DB_CONFIG = {
    "host": "your-rds-proxy-endpoint.rds.amazonaws.com",
    "port": 5432,
    "user": "your_user",
    "password": "your_password",
    "dbname": "your_dbname",
    "connect_timeout": 5,
}

CLIENT_COUNT = 5           # Number of clients (adjustable)
RETRY_INTERVAL = 2         # Seconds between retries
QUERY_INTERVAL = 1         # Seconds between operations
CSV_LOG_FILE = "failover_dml_log.csv"

# Lock for file and counter access
csv_lock = threading.Lock()
counter_lock = threading.Lock()

# DML success/failure counters
dml_counters = defaultdict(lambda: {"insert_success": 0, "insert_fail": 0, "update_success": 0, "update_fail": 0})

def log_event(client_id, event_type, message):
    timestamp = datetime.now().isoformat()
    print(f"[{timestamp}] Client-{client_id} | {event_type.upper()}: {message}")
    with csv_lock:
        with open(CSV_LOG_FILE, mode="a", newline="") as file:
            writer = csv.writer(file)
            writer.writerow([timestamp, f"Client-{client_id}", event_type, message])

def connect(client_id):
    while True:
        try:
            conn = psycopg2.connect(**DB_CONFIG)
            conn.autocommit = True
            log_event(client_id, "connected", "Connection established")
            return conn
        except Exception as e:
            log_event(client_id, "connection_failed", str(e))
            time.sleep(RETRY_INTERVAL)

def run_dml_operations(cur, client_id):
    try:
        # INSERT
        cur.execute("INSERT INTO test_failover (client_id, created_at) VALUES (%s, now());", (client_id,))
        with counter_lock:
            dml_counters[client_id]["insert_success"] += 1
        log_event(client_id, "insert_success", "Inserted one row")

        # UPDATE (we try to update the most recent row for this client)
        cur.execute("""
            UPDATE test_failover 
            SET updated_at = now() 
            WHERE client_id = %s 
            ORDER BY created_at DESC 
            LIMIT 1;
        """, (client_id,))
        with counter_lock:
            dml_counters[client_id]["update_success"] += 1
        log_event(client_id, "update_success", "Updated latest row")

    except Exception as e:
        log_event(client_id, "dml_failed", str(e))
        with counter_lock:
            dml_counters[client_id]["insert_fail"] += 1
            dml_counters[client_id]["update_fail"] += 1
        raise e  # Let the caller handle reconnect logic

def client_thread(client_id):
    conn = connect(client_id)
    cur = conn.cursor()

    while True:
        try:
            cur.execute("SELECT now();")
            now = cur.fetchone()[0]
            log_event(client_id, "query_success", str(now))

            run_dml_operations(cur, client_id)

            time.sleep(QUERY_INTERVAL)

        except Exception as e:
            log_event(client_id, "query_failed", str(e))
            try:
                cur.close()
                conn.close()
            except:
                pass
            log_event(client_id, "reconnect", "Attempting to reconnect")
            time.sleep(RETRY_INTERVAL)
            conn = connect(client_id)
            cur = conn.cursor()

def summary_printer():
    while True:
        time.sleep(10)  # Print summary every 10 seconds
        with counter_lock:
            print("\n===== DML Summary =====")
            for client_id, counts in dml_counters.items():
                print(f"Client-{client_id}: "
                      f"INSERT ✓ {counts['insert_success']} ✗ {counts['insert_fail']} | "
                      f"UPDATE ✓ {counts['update_success']} ✗ {counts['update_fail']}")
            print("=======================\n")

def run_test():
    # Create table (if not exists)
    try:
        init_conn = psycopg2.connect(**DB_CONFIG)
        with init_conn:
            with init_conn.cursor() as cur:
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS test_failover (
                        id SERIAL PRIMARY KEY,
                        client_id INT,
                        created_at TIMESTAMP,
                        updated_at TIMESTAMP
                    );
                """)
        init_conn.close()
    except Exception as e:
        print(f"Failed to initialize table: {e}")
        return

    # Init CSV
    with open(CSV_LOG_FILE, mode="w", newline="") as file:
        writer = csv.writer(file)
        writer.writerow(["timestamp", "client", "event_type", "message"])

    # Start client threads
    threads = []
    for client_id in range(1, CLIENT_COUNT + 1):
        t = threading.Thread(target=client_thread, args=(client_id,), daemon=True)
        threads.append(t)
        t.start()

    # Start summary printer thread
    summary_thread = threading.Thread(target=summary_printer, daemon=True)
    summary_thread.start()

    # Keep main thread alive
    while True:
        time.sleep(10)

if __name__ == "__main__":
    run_test()
